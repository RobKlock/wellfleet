# ASOS Uncertainty Model

## The Problem

You placed a bet on Denver minimum temperature being in the 18-19Â°F range (market B18.5) on Jan 19, 2026. The real-time METAR observations showed **19Â°F** multiple times throughout the morning (5:30 AM, 5:35 AM, 7:25 AM, 9:30 AM, etc.). However, the preliminary NWS Climate Report issued at 7:30 AM showed the minimum as **20Â°F** at 5:35 AM.

**This discrepancy is critical for betting** - if the final Climate Report shows 20Â°F instead of 19Â°F, your bet loses!

## Why This Happens: ASOS 5-Minute Averaging

Based on official NWS documentation, we learned that:

### ASOS Doesn't Report Instantaneous Temperatures

> "Temperature is measured every minute with a 5-minute running average. The system maintains running maximum and minimum values that are updated every minute throughout the 24-hour period."

**Key insight:** The displayed "19Â°F" in METAR observations is a **rounded 5-minute average**, not an instantaneous reading.

### The Rounding Problem

When you see "19Â°F" displayed:
- The actual 5-minute average could be **18.5-19.4Â°F** â†’ rounds to 19Â°F (good for your bet!)
- OR it could be **19.5-19.9Â°F** â†’ also displays as 19Â°F but rounds to **20Â°F** in the final CLI (bad!)

**You can't tell which without seeing the raw sensor data.**

### Quality Control Makes It Worse

The NWS Climate Report undergoes three levels of quality control:
1. **Level 1 (Real-time)**: Automated self-diagnostics, rate-of-change checks
2. **Level 2 (Area, 1-2 hours)**: WFO personnel review for consistency
3. **Level 3 (National, ~2 hours)**: NCDC performs additional QC before archiving

Observations can be flagged, adjusted, or removed at any level. The **Daily Summary Message (DSM)** used for the preliminary CLI is considered more reliable than individual METAR observations.

## The Solution: ASOS Uncertainty Model

We've implemented a model that accounts for this uncertainty:

### 1. Uncertainty Zone Detection

When an observed temperature is within **Â±1Â°F** of a market threshold, we flag it as being in the "ASOS uncertainty zone":

```python
ASOS_UNCERTAINTY_RANGE = 1.0  # Â±1Â°F uncertainty zone

# Example: Observed 19.4Â°F, threshold 20Â°F
distance_to_threshold = abs(19.4 - 20.0) = 0.6Â°F
# 0.6Â°F < 1.0Â°F â†’ IN UNCERTAINTY ZONE! âš ï¸
```

### 2. Confidence Reduction

When in the uncertainty zone, we reduce confidence by **30%**:

```python
base_confidence = 0.95  # 95% if clearly resolved
confidence = base_confidence * (1 - 0.30)  # Reduce by 30%
# confidence = 0.665 = 66.5%
```

This accounts for the possibility that the displayed value might round differently in the final Climate Report.

### 3. Warning Messages

The scanner issues clear warnings:

```
âš ï¸ ASOS UNCERTAINTY: Observed 19.4Â°F is within 0.6Â°F of threshold 20Â°F.
Displayed value may round differently in final CLI!
```

## Real Example: Denver Jan 19, 2026

**Market:** B18.5 (18-19Â°F range)
**Observed:** 19.4Â°F throughout morning
**Preliminary CLI:** 20Â°F at 5:35 AM

### Without ASOS Model (OLD)
```
Observed: 19.4Â°F in range [18-19Â°F]
Past peak time (8am): YES
Confidence: 95% â†’ Bet YES with high conviction âœ…
```

### With ASOS Model (NEW)
```
Observed: 19.4Â°F near range boundary [18-19Â°F]
âš ï¸ ASOS UNCERTAINTY WARNING!
Distance to boundary: 0.6Â°F (within uncertainty zone)
Confidence: 66.5% (reduced from 95%)
âš ï¸ RISK: If 5-min avg was 19.5-19.9Â°F, final CLI will show 20Â°F â†’ BET LOSES
```

## When to Trust Observations

### High Confidence (Safe to Bet)

**Observed value is >1Â°F away from threshold:**
```
Observed: 17.0Â°F, Threshold: 20Â°F
Distance: 3.0Â°F â†’ NO UNCERTAINTY
Confidence: 95% â†’ Safe bet âœ…
```

**Past peak time + clearly outside range:**
```
Observed: 15.0Â°F for market "20-21Â°F"
Clearly below range, past 8am
Confidence: 95% â†’ Safe bet âœ…
```

### Low Confidence (Risky Bet)

**Observed value within Â±1Â°F of threshold:**
```
Observed: 19.4Â°F, Threshold: 20Â°F
Distance: 0.6Â°F â†’ UNCERTAINTY ZONE âš ï¸
Confidence: 66.5% â†’ Risky!
```

**Observed value right on boundary:**
```
Observed: 20.0Â°F for range "20-21Â°F"
On lower boundary â†’ UNCERTAINTY ZONE âš ï¸
Could round to 19Â°F or 21Â°F depending on decimals
Confidence: 66.5% â†’ Risky!
```

### Zero Confidence (Don't Bet)

**Before peak time:**
```
Current time: 6:00 AM (before 8am minimum peak)
Temperature could still drop â†’ DON'T BET
```

**No observations yet:**
```
Market opens but no observations from today
Pure forecast, no constraints â†’ DON'T BET
```

## Using the Model

### Run Scanner with ASOS Warnings

```bash
python show_all_markets_verbose.py KXLOWTDEN
```

Look for these warning signs:
```
âš ï¸ ASOS UNCERTAINTY: Observed 19.4Â°F is within 0.6Â°F of threshold 20.0Â°F.
Displayed value may round differently in final CLI!

Observed minimum 19.4Â°F in range [18.0Â°F, 19.0Â°F] and past peak time â†’ Definite YES (confidence: 66.5%)
```

### Interpret Confidence Levels

| Confidence | Interpretation | Action |
|------------|----------------|--------|
| 95% | Clearly resolved, >1Â°F from boundary | âœ… High conviction bet |
| 66.5% | In uncertainty zone, ASOS rounding risk | âš ï¸ Reduced bet or hedge |
| 50% | Before peak time, outcome unclear | âŒ Don't bet yet |
| 5% | Opposite of observation, very unlikely | âœ… High conviction opposite side |

### Test the Model

```bash
python test_asos_uncertainty.py
```

This runs test cases showing how the model handles:
- Values far from threshold (no uncertainty)
- Values near threshold (uncertainty warning)
- Values on boundaries (maximum uncertainty)
- Real Denver Jan 19 case

## Validation Tomorrow

**Tomorrow (Jan 20) around 7:30 AM MST**, check the final Climate Report:

```
https://forecast.weather.gov/product.php?site=BOU&product=CLI&issuedby=DEN
```

Compare the final minimum against what you bet on:
- **If final shows 19Â°F:** âœ… Model was conservative, bet wins!
- **If final shows 20Â°F:** âš ï¸ Model correctly warned of uncertainty, bet loses

This real-world validation will help us calibrate the uncertainty threshold (currently Â±1Â°F) and confidence reduction (currently 30%).

## Future Improvements

### 1. Preliminary CLI Validation

We've added a method to fetch preliminary CLI reports:

```python
preliminary = nws.get_preliminary_climate_report("KDEN", "2026-01-19")
# Returns: {"preliminary_min": 20.0, "min_time": "535 AM"}
```

**Future enhancement:** When preliminary CLI differs from observations, trust the CLI more since it's based on the Daily Summary Message after quality control.

### 2. Calibration from Historical Data

After collecting real outcomes:
- If 66.5% confidence â†’ 95% win rate: too conservative, reduce penalty
- If 66.5% confidence â†’ 50% win rate: correct calibration âœ…
- If 66.5% confidence â†’ 30% win rate: too aggressive, increase penalty

### 3. Station-Specific Adjustments

Different ASOS stations might have different rounding behaviors or quality control patterns. We could learn station-specific uncertainty parameters from historical data.

## Key Takeaways

1. **ASOS uses 5-minute averages**, not instantaneous readings
2. **Displayed integers mask underlying decimals** that determine final rounding
3. **Â±1Â°F from threshold = uncertainty zone** where rounding could go either way
4. **Reduce confidence by 30%** when in uncertainty zone
5. **Preliminary CLI is more reliable** than individual METAR observations
6. **Validate tomorrow** to see if model correctly predicted the risk

## The Houston Precedent

Remember the Reddit user who saw **71Â°F all day** (actually 70.7Â°F in observations) but the final Climate Report said only **70Â°F**? This is exactly the ASOS uncertainty problem we're now modeling!

Your Denver Jan 19 bet is a perfect test case. Check tomorrow's final report and let us know what it shows! ğŸ“Š
